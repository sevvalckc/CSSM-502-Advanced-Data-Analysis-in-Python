# -*- coding: utf-8 -*-
"""CSSM502_Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Q7qxyCFeHtTPkZKsrOONRO2XhmvImpn0

# CSSM502 Final Project

## Dataset Preparations
"""

from google.colab import files
files.upload()

!mkdir -p /root/.kaggle
!cp kaggle.json /root/.kaggle/kaggle.json
!chmod 600 /root/.kaggle/kaggle.json

!ls -lah /root/.kaggle
!kaggle --version

!kaggle datasets list -s "reddit comments" | head -n 25

!rm -rf data_en
!mkdir -p data_en
!kaggle datasets download -d smagnan/1-million-reddit-comments-from-40-subreddits -p data_en --unzip
!find data_en -type f | head -n 50

import glob, pandas as pd

en_files = glob.glob("data_en/**/*.csv", recursive=True)
print("CSV files:", en_files)

df_en_raw = pd.read_csv(en_files[0])
print("df_en_raw shape:", df_en_raw.shape)
print("columns:", list(df_en_raw.columns)[:30])
df_en_raw.head(2)

text_candidates = [c for c in df_en_raw.columns if c.lower() in ["body","text","comment","comments","content"]]
sub_candidates  = [c for c in df_en_raw.columns if "sub" in c.lower()]  # subreddit / subreddit_id vs.

print("text candidates:", text_candidates)
print("sub candidates:", sub_candidates)

TEXT_COL = text_candidates[0]
SUB_COL  = sub_candidates[0] if sub_candidates else None

df_en_raw[TEXT_COL] = df_en_raw[TEXT_COL].fillna("").astype(str)

en_kw = [
    "work","job","boss","manager","overtime","burnout","stress","toxic",
    "salary","wage","pay","harassment","anxiety","depressed","quit",
    "resign","resignation","fired","laid off","workload","exhaust"
]
pattern = "|".join([k.replace(" ", "\\s+") for k in en_kw])

df_en_pool = df_en_raw[df_en_raw[TEXT_COL].str.lower().str.contains(pattern, na=False)].copy()
df_en_pool["text"] = df_en_pool[TEXT_COL].str.strip()


df_en_pool = df_en_pool[df_en_pool["text"].str.len() >= 30]


if SUB_COL:
    workish = ["antiwork","jobs","careeradvice","work","burnout","depression","anxiety"]
    df_en_pool = df_en_pool[
        df_en_pool[SUB_COL].astype(str).str.lower().isin(workish) |
        df_en_pool[TEXT_COL].str.lower().str.contains(pattern, na=False)
    ].copy()

print("EN pool size:", df_en_pool.shape)


if len(df_en_pool) >= 10000:
    df_en_10k = df_en_pool.sample(10000, random_state=42)
else:

    broad_kw = ["tired","exhausted","hate my job","money","rent","bills","career","unemployed","overworked"]
    broad_pat = "|".join([k.replace(" ", "\\s+") for k in broad_kw])
    df_en_pool2 = df_en_raw[df_en_raw[TEXT_COL].str.lower().str.contains(broad_pat, na=False)].copy()
    df_en_pool2["text"] = df_en_pool2[TEXT_COL].fillna("").astype(str).str.strip()
    df_en_pool2 = df_en_pool2[df_en_pool2["text"].str.len() >= 30]
    df_en_10k = pd.concat([df_en_pool, df_en_pool2], ignore_index=True).drop_duplicates(subset=["text"]).head(10000)

print("EN final (raw) shape:", df_en_10k.shape)
df_en_10k[["text"]].head(3)

"""## Preprocessing"""

import re
import pandas as pd

def clean_text_basic(s: str) -> str:
    if not isinstance(s, str):
        return ""
    s = s.strip()

    # remove deleted/removed placeholders
    if s.lower() in ["[deleted]", "[removed]"]:
        return ""

    # remove URLs
    s = re.sub(r"http\S+|www\.\S+", " ", s)

    # remove excessive markdown quotes (lines starting with >)
    s = re.sub(r"(?m)^\s*>\s?.*$", " ", s)

    # normalize whitespace
    s = re.sub(r"\s+", " ", s).strip()

    return s

df_en_pool_clean = df_en_pool.copy()
df_en_pool_clean["text_clean"] = df_en_pool_clean["text"].apply(clean_text_basic)

df_en_pool_clean = df_en_pool_clean[df_en_pool_clean["text_clean"].str.len() >= 50].copy()

df_en_pool_clean = df_en_pool_clean.drop_duplicates(subset=["text_clean"])

print("EN pool before:", df_en_pool.shape)
print("EN pool after clean:", df_en_pool_clean.shape)

df_en_10k_clean = df_en_pool_clean.sample(min(10000, len(df_en_pool_clean)), random_state=42).copy()

df_en_10k_clean = df_en_10k_clean.rename(columns={"text_clean": "text"})
df_en_10k_clean = df_en_10k_clean[["text"] + [c for c in df_en_10k_clean.columns if c != "text"]]

print("EN final clean 10k:", df_en_10k_clean.shape)
df_en_10k_clean[["text"]].head(3)

df_en_10k_clean.to_csv("EN_clean_10k.csv", index=False)
print("Saved EN_clean_10k.csv rows:", len(df_en_10k_clean))

df_final = df_en_10k_clean.copy()

print("Final EN shape:", df_final.shape)
df_final.head(3)

df_final = df_final.copy()

df_final["text"] = df_final["text"].astype(str)

df_final = df_final.drop(columns=["body"], errors="ignore")

print(df_final.shape)
df_final.head(2)

df_final.columns

txt = df_final.loc[:, "text"]
df_final = df_final.drop(columns=["text"])

df_final["text"] = txt.iloc[:, 0].astype(str)

print(df_final.columns)
print(type(df_final["text"]))
df_final.head(3)

en_kw = [
    "work","job","boss","manager","office","career","salary","wage",
    "rent","bills","money","economy","inflation",
    "burnout","stress","tired","exhausted","overwork","mental health",
    "unemployment","laid off","pay","raise","promotion"
]
pattern_en = "|".join(en_kw)

coverage = df_final["text"].str.lower().str.contains(pattern_en, na=False).mean()
coverage

noise_kw = ["movie","film","actor","episode","season","game","match","goal","team","product","amazon","delivery"]
noise_pattern = "|".join(noise_kw)

noise_ratio = df_final["text"].str.lower().str.contains(noise_pattern, na=False).mean()
noise_ratio

"""## Text Embedding"""

!pip -q install sentence-transformers

from sentence_transformers import SentenceTransformer
import numpy as np

# Pre-trained sentence embedding model
model = SentenceTransformer("all-MiniLM-L6-v2")

texts = df_final["text"].tolist()

embeddings = model.encode(
    texts,
    batch_size=64,
    show_progress_bar=True,
    normalize_embeddings=True
)

print("embeddings.shape:", embeddings.shape)

"""## DIMENSIONALITY REDUCTION (UMAP)"""

!pip -q install umap-learn

import umap
import matplotlib.pyplot as plt

umap_model = umap.UMAP(
    n_neighbors=15,
    min_dist=0.05,
    metric="cosine",
    random_state=42
)

umap_2d = umap_model.fit_transform(embeddings)

print(umap_2d.shape)

df_final["x"] = umap_2d[:, 0]
df_final["y"] = umap_2d[:, 1]

df_final[["x","y"]].head()

plt.figure(figsize=(8,6))
plt.scatter(df_final["x"], df_final["y"], s=6, alpha=0.4)
plt.title("UMAP projection of English Reddit texts")
plt.xlabel("UMAP-1")
plt.ylabel("UMAP-2")
plt.show()

"""## Clustering"""

from sklearn.cluster import KMeans

k = 8

kmeans = KMeans(
    n_clusters=k,
    random_state=42,
    n_init="auto"
)

clusters = kmeans.fit_predict(embeddings)

df_final["cluster"] = clusters

df_final["cluster"].value_counts()

plt.figure(figsize=(8,6))
plt.scatter(
    df_final["x"],
    df_final["y"],
    c=df_final["cluster"],
    cmap="tab10",
    s=6,
    alpha=0.6
)
plt.title("UMAP projection colored by KMeans clusters")
plt.xlabel("UMAP-1")
plt.ylabel("UMAP-2")
plt.colorbar(label="Cluster")
plt.show()

import numpy as np

centers = kmeans.cluster_centers_
centers = centers / np.linalg.norm(centers, axis=1, keepdims=True)

def get_representative_texts(cluster_id, n=5):
    idx = np.where(df_final["cluster"] == cluster_id)[0]
    sims = embeddings[idx] @ centers[cluster_id]
    top_idx = idx[np.argsort(-sims)[:n]]
    return df_final.iloc[top_idx][["text", "subreddit", "score"]]

# Ã–rnek: cluster 0
get_representative_texts(0, n=5)

for c in sorted(df_final["cluster"].unique()):
    print(f"\n=== CLUSTER {c} ===")
    display(get_representative_texts(c, n=5))

"""## Focused Analysis: Work-Related Discourse"""

WORK_CLUSTERS = [0, 4, 5]

df_work = df_final[df_final["cluster"].isin(WORK_CLUSTERS)].copy()

print("Work-related subset shape:", df_work.shape)
df_work["cluster"].value_counts()

coverage_work = df_work["text"].str.lower().str.contains(pattern_en, na=False).mean()
noise_work = df_work["text"].str.lower().str.contains(noise_pattern, na=False).mean()

coverage_work, noise_work

plt.figure(figsize=(8,6))
plt.scatter(
    df_work["x"],
    df_work["y"],
    c=df_work["cluster"],
    cmap="tab10",
    s=8,
    alpha=0.7
)
plt.title("UMAP projection of work-related Reddit discourse")
plt.xlabel("UMAP-1")
plt.ylabel("UMAP-2")
plt.colorbar(label="Cluster")
plt.show()

import pandas as pd

summary = {
    "EN_pool_raw": [df_en_raw.shape[0]],
    "EN_pool_filtered": [df_en_pool.shape[0]],
    "EN_pool_clean": [df_en_pool_clean.shape[0]],
    "EN_sampled_for_embedding": [df_final.shape[0]],  # 10,000
    "Coverage_keyword_rate": [float(coverage)],
    "Noise_keyword_rate": [float(noise_ratio)],
    "KMeans_k": [k],
    "Work_clusters": [str(WORK_CLUSTERS)],
    "Work_subset_size": [df_work.shape[0]],
    "Work_coverage_rate": [float(coverage_work)],
    "Work_noise_rate": [float(noise_work)],
}

df_summary = pd.DataFrame(summary)
df_summary

import matplotlib.pyplot as plt

cluster_counts = df_final["cluster"].value_counts().sort_index()

plt.figure(figsize=(7,4))
plt.bar(cluster_counts.index.astype(str), cluster_counts.values)
plt.title("Cluster size distribution (KMeans, k=8)")
plt.xlabel("Cluster")
plt.ylabel("Number of texts")
plt.show()

cluster_counts

rows = []
for c in range(k):
    reps = get_representative_texts(c, n=10).copy()
    reps["cluster"] = c
    rows.append(reps)

df_reps = pd.concat(rows, ignore_index=True)
df_reps.to_csv("cluster_representatives_top10.csv", index=False)
print("Saved: cluster_representatives_top10.csv", df_reps.shape)

df_work_out = df_work[["text","subreddit","score","cluster","x","y"]].copy()
df_work_out.to_csv("EN_work_subset_3330.csv", index=False)
print("Saved: EN_work_subset_3330.csv", df_work_out.shape)

import numpy as np

work_terms = ["work","job","boss","salary","wage","pay","rent","bills","burnout","stress","overtime","fired","laid off"]
pattern_work = "|".join([t.replace(" ", "\\s+") for t in work_terms])

cluster_diag = (
    df_final.assign(has_work=df_final["text"].str.lower().str.contains(pattern_work, na=False))
           .groupby("cluster")["has_work"]
           .mean()
           .sort_values(ascending=False)
)

cluster_diag