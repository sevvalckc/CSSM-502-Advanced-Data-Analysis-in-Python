# -*- coding: utf-8 -*-
"""CSSM502_Hw6.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VIY1-KLyGt64arxjR7OakqOKNh50IwD4
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

path = "/content/drive/MyDrive/CSSM502/Assignment 6/Dataset/adult income.csv"
df = pd.read_csv(path)
df.head()

import pandas as pd

print("Shape:", df.shape)
print("\nIncome distribution:")
print(df["income"].value_counts(dropna=False))
print("\nIncome distribution (ratio):")
print(df["income"].value_counts(normalize=True, dropna=False))

print("\nHow many '?' per column:")
q_counts = (df == "?").sum().sort_values(ascending=False)
print(q_counts[q_counts > 0])

print("\nColumns:", df.columns.tolist())

df_clean = df.replace("?", pd.NA)

df_clean = df_clean.dropna()

# make income target binary encode
df_clean["income"] = df_clean["income"].map({"<=50K": 0, ">50K": 1})

print("New shape:", df_clean.shape)
print("\nIncome distribution after cleaning:")
print(df_clean["income"].value_counts(normalize=True))

# Target
y = df_clean["income"]
X = df_clean.drop("income", axis=1)

# Numerical & categorical columns
numerical_cols = X.select_dtypes(include=["int64", "float64"]).columns.tolist()
categorical_cols = X.select_dtypes(include=["object"]).columns.tolist()

print("Numerical columns:", numerical_cols)
print("\nCategorical columns:", categorical_cols)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,
    random_state=42,
    stratify=y
)

# Preprocessing
preprocessor = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), numerical_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_cols)
    ]
)

print("Train shape:", X_train.shape)
print("Test shape:", X_test.shape)

from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, classification_report

# Pipeline
log_reg_pipeline = Pipeline(
    steps=[
        ("preprocess", preprocessor),
        ("model", LogisticRegression(max_iter=1000))
    ]
)

# Fit
log_reg_pipeline.fit(X_train, y_train)

# Predict
y_pred_lr = log_reg_pipeline.predict(X_test)

# Evaluation
acc_lr = accuracy_score(y_test, y_pred_lr)
f1_lr = f1_score(y_test, y_pred_lr)

print("Logistic Regression Results")
print("Accuracy:", acc_lr)
print("F1-score:", f1_lr)
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred_lr))

from sklearn.tree import DecisionTreeClassifier

# Decision Tree pipeline
dt_pipeline = Pipeline(
    steps=[
        ("preprocess", preprocessor),
        ("model", DecisionTreeClassifier(
            random_state=42
        ))
    ]
)

# Fit
dt_pipeline.fit(X_train, y_train)

# Predict
y_pred_dt = dt_pipeline.predict(X_test)

# Evaluation
acc_dt = accuracy_score(y_test, y_pred_dt)
f1_dt = f1_score(y_test, y_pred_dt)

print("Decision Tree Results")
print("Accuracy:", acc_dt)
print("F1-score:", f1_dt)
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred_dt))

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.pipeline import Pipeline

rf_pipeline = Pipeline(
    steps=[
        ("preprocess", preprocessor),
        ("model", RandomForestClassifier(
            random_state=42,
            n_jobs=-1
        ))
    ]
)

# smaller grid
param_grid_fast = {
    "model__n_estimators": [200],
    "model__max_depth": [None, 20],
    "model__min_samples_split": [2, 5]
}


grid_search_fast = GridSearchCV(
    rf_pipeline,
    param_grid_fast,
    cv=3,
    scoring="f1",
    n_jobs=-1,
    verbose=1
)

grid_search_fast.fit(X_train, y_train)

print("Best parameters:", grid_search_fast.best_params_)
print("Best CV F1-score:", grid_search_fast.best_score_)

from sklearn.metrics import accuracy_score, f1_score, classification_report

best_rf = grid_search_fast.best_estimator_

y_pred_rf = best_rf.predict(X_test)

acc_rf = accuracy_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)

print("Random Forest (Tuned) Results")
print("Accuracy:", acc_rf)
print("F1-score:", f1_rf)
print("\nClassification Report:\n")
print(classification_report(y_test, y_pred_rf))

import pandas as pd

results = pd.DataFrame({
    "Model": ["Logistic Regression", "Decision Tree", "Random Forest (Tuned)"],
    "Accuracy": [acc_lr, acc_dt, acc_rf],
    "F1-score": [f1_lr, f1_dt, f1_rf]
})

results